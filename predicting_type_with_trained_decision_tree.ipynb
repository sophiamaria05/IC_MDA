{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJiid41hqCRo1kyBxGKP4z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophiamaria05/IC_MDA/blob/main/predicting_type_with_trained_decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing"
      ],
      "metadata": {
        "id": "MF32B7Twgbpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing libraries"
      ],
      "metadata": {
        "id": "AQ4r6lb98OjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reading xml files\n",
        "import xml.etree.cElementTree as ET\n",
        "from xml.dom.minidom import parseString\n",
        "#deal with trees\n",
        "from sklearn import tree\n",
        "import joblib\n",
        "#deal with tables\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#importing files\n",
        "import os"
      ],
      "metadata": {
        "id": "97aQHXxH7YU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining methods"
      ],
      "metadata": {
        "id": "crVNIBth8cIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***get_xml_files(xml_file_folder):*** Geting the xml files (of the phrases)\n",
        "def get_xml_files(xml_file_folder):\n",
        "  if len([file for file in os.listdir(xml_file_folder) if file.endswith('.xml')])==0:\n",
        "    raise FileNotFoundError(\"No xml file was found! Please upload the file with the tokenized phrases.\")\n",
        "  return [file for file in os.listdir(xml_file_folder) if file.endswith('.xml')]"
      ],
      "metadata": {
        "id": "WcxqFi8m8igB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***get_tokens(xml_file):*** Gets the tokens from the xml\n",
        "def get_tokens(xml_file):\n",
        "  tree_xml = ET.parse(xml_file)\n",
        "  root_xml = tree_xml.getroot()\n",
        "  return tree_xml, root_xml"
      ],
      "metadata": {
        "id": "iYLHggpT7-9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***get_table(tree_xml, root_xml, reserved_words):*** Gets table from xml\n",
        "def get_table(tree_xml, root_xml, reserved_words):\n",
        "  phrases = {'phrase': [], 'id_use_case':[], 'flow':[], 'id_phrase':[], 'noun':[],       'verb':[],       'object':[],       'complement':[],       'complementN':[],\n",
        "                'noun_is_system':[], 'reserved_word':[], 'noun_check':[], 'verb_check':[], 'object_check':[], 'complement_check':[], 'complementN_check':[]}\n",
        "  for use_case in root_xml:\n",
        "    for flow in use_case:\n",
        "      for phrase in flow:\n",
        "        #print(phrases)#FIXING \"All arrays must be of the same length\" ERROR\n",
        "        if (phrase.get('type') != None):\n",
        "          #print(\"type:\", phrase.get('type'))#FIXING \"All arrays must be of the same length\" ERROR\n",
        "          continue\n",
        "        elif ((phrase.find(\"noun\") is None) or (phrase.find(\"verb\") is None) or (phrase.find(\"object\") is None)):\n",
        "          #print(\"type:\", phrase.get('type'))#FIXING \"All arrays must be of the same length\" ERROR\n",
        "          phrase.set('type', '0')\n",
        "          continue\n",
        "\n",
        "        #append phrase\n",
        "        if phrase.text != None:\n",
        "          phrases['phrase'].append(phrase.text)\n",
        "        else:\n",
        "          phrases['phrase'].append(np.nan)\n",
        "\n",
        "        #append the ids to find the phrase\n",
        "        phrases['id_use_case'].append(use_case.get(\"id\"))\n",
        "        phrases['flow'].append(flow.tag)\n",
        "        phrases['id_phrase'].append(phrase.get(\"id\"))\n",
        "\n",
        "        #append noun\n",
        "        '''\n",
        "        if phrase.find(\"noun\") is not None:\n",
        "          if phrase.find(\"noun\").text==\"Sistema\":\n",
        "            phrases['noun_is_system'].append(1)\n",
        "          else:\n",
        "            phrases['noun_is_system'].append(0)\n",
        "          phrases['noun_check'].append(1)\n",
        "          phrases['noun'].append(phrase.find(\"noun\").text)\n",
        "        else:\n",
        "          phrases['noun_check'].append(0)\n",
        "          phrases['noun'].append(np.nan)\n",
        "\n",
        "          phrases['noun_is_system'].append(0)#FIXING \"All arrays must be of the same length\" ERROR\n",
        "        '''\n",
        "        phrases['noun_check'].append(1)\n",
        "        phrases['noun'].append(phrase.find(\"noun\").text)\n",
        "        if phrase.find(\"noun\").text==\"Sistema\":\n",
        "          phrases['noun_is_system'].append(1)\n",
        "        else:\n",
        "          phrases['noun_is_system'].append(0)\n",
        "\n",
        "        #append verb\n",
        "        '''\n",
        "        if phrase.find(\"verb\") is not None:\n",
        "          phrases['verb_check'].append(1)\n",
        "          phrases['verb'].append(phrase.find(\"verb\").text)\n",
        "          if phrase.find(\"verb\").text in reserved_words:\n",
        "            phrases['reserved_word'].append(1)\n",
        "          else:\n",
        "            phrases['reserved_word'].append(0)\n",
        "        else:\n",
        "          phrases['verb_check'].append(0)\n",
        "          phrases['verb'].append(np.nan)\n",
        "\n",
        "          phrases['reserved_word'].append(0)#FIXING \"All arrays must be of the same length\" ERROR\n",
        "        '''\n",
        "        phrases['verb_check'].append(1)\n",
        "        phrases['verb'].append(phrase.find(\"verb\").text)\n",
        "        if phrase.find(\"verb\").text in reserved_words:\n",
        "          phrases['reserved_word'].append(1)\n",
        "        else:\n",
        "          phrases['reserved_word'].append(0)\n",
        "\n",
        "        #append object\n",
        "        '''\n",
        "        if phrase.find(\"object\") is not None:\n",
        "          phrases['object_check'].append(1)\n",
        "          phrases['object'].append(phrase.find(\"object\").text)\n",
        "        else:\n",
        "          phrases['object_check'].append(0)\n",
        "          phrases['object'].append(np.nan)\n",
        "        '''\n",
        "        phrases['object_check'].append(1)\n",
        "        phrases['object'].append(phrase.find(\"object\").text)\n",
        "\n",
        "        #append complement\n",
        "        if phrase.find(\"complement\") is not None:\n",
        "          phrases['complement_check'].append(1)\n",
        "          phrases['complement'].append(phrase.find(\"complement\").text)\n",
        "        else:\n",
        "          phrases['complement_check'].append(0)\n",
        "          phrases['complement'].append(np.nan)\n",
        "\n",
        "        #append complementN\n",
        "        if phrase.find(\"complementN\") is not None:\n",
        "          phrases['complementN_check'].append(1)\n",
        "          phrases['complementN'].append(phrase.find(\"complementN\").text)\n",
        "        else:\n",
        "          phrases['complementN_check'].append(0)\n",
        "          phrases['complementN'].append(np.nan)\n",
        "\n",
        "  #print('\\n\\n\\n',phrases)#FIXING \"All arrays must be of the same length\" ERROR\n",
        "\n",
        "  df = pd.DataFrame.from_dict(phrases)\n",
        "  return df"
      ],
      "metadata": {
        "id": "z0Y76b0v85s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***get_trained_tree(tree_file_folder):*** Gets trained decision tree from the joblib file\n",
        "def get_trained_tree(tree_file_folder):\n",
        "  tree_file = [file for file in os.listdir(tree_file_folder) if file.endswith('.joblib')]\n",
        "  if len(tree_file)==0:\n",
        "    raise FileNotFoundError(\"No decision tree model was found! Please upload the joblib file.\")\n",
        "  return joblib.load(tree_file_folder+\"/\"+tree_file[0])"
      ],
      "metadata": {
        "id": "YGxdA_GNAAfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***int_to_tag(tag_int):*** Converts the ***type*** back to tag\n",
        "def int_to_tag(tag_int):\n",
        "  if type(tag_int)!=int:\n",
        "    return \"\"\n",
        "  if tag_int==0:\n",
        "    return \"0\"\n",
        "  tag = \"\"\n",
        "  while tag_int!=1:\n",
        "    tag+=(str(tag_int%2)+\".\")\n",
        "    tag_int = tag_int//2\n",
        "  tag+=\"1\"\n",
        "  return tag"
      ],
      "metadata": {
        "id": "OWiC1PC07har"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***predict_types(df):*** Predicts the type of the phrases using the trained tree\n",
        "def predict_types(dtc, df):\n",
        "  types = dtc.predict(df.iloc[:, -7:])\n",
        "  return types"
      ],
      "metadata": {
        "id": "Qc5W7kB0_qaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***concat_types_to_df(df, types):*** Adds the predicted types to the data frame\n",
        "def concat_types_to_df(df, types):\n",
        "  types_tag = []\n",
        "  for tag in types:\n",
        "    types_tag.append(int_to_tag(int(tag)))\n",
        "  df = pd.concat([df, pd.DataFrame(types_tag, columns=['types'])], axis=1)\n",
        "  return df"
      ],
      "metadata": {
        "id": "DzL2pABQ7BSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***name_file(save_folder, xml_file):*** Defines the name of the written file\n",
        "def name_file(save_folder, xml_file):\n",
        "  if '.xml' == xml_file[-4:]:\n",
        "    xml_file = xml_file[:-4]\n",
        "  if \"tokenized_phrases (\" == xml_file[:19] and \")\" == xml_file[-1:]:\n",
        "    return save_folder+'predicted_phrases_types ('+xml_file[19:-1]+').xml'\n",
        "  else:\n",
        "    return save_folder+'predicted_phrases_types ('+xml_file+').xml'\n",
        "\n",
        "#EXAMPLES:\n",
        "# save_folder = \"/content/predicted_files/\"\n",
        "# xml_file = \"tokenized_phrases (arquivo (3)).xml\"\n",
        "# name_file(save_folder, xml_file) #/content/predicted_files/predicted_phrases_types (arquivo (3)).xml\n",
        "\n",
        "# save_folder = \"/content/predicted_files/\"\n",
        "# xml_file = \"arquivo (3).xml\"\n",
        "# name_file(save_folder, xml_file) #/content/predicted_files/predicted_phrases_types (arquivo (3)).xml"
      ],
      "metadata": {
        "id": "VRYcIRfgLG6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main method"
      ],
      "metadata": {
        "id": "XnlMkWO6_UAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ***predict(tree_file_folder, reserved_words, xml_file_folder):***\n",
        "def predict(tree_file_folder, reserved_words, tokenized_folder, save_folder):\n",
        "  #Gets phrases\n",
        "  xml_files = get_xml_files(tokenized_folder)\n",
        "  #Predicts and get dataframe for each xml file\n",
        "  i=0\n",
        "  predicted_xmls = []\n",
        "  for xml_file in xml_files:\n",
        "    tree_xml, root_xml = get_tokens(tokenized_folder+'/'+xml_file)\n",
        "    df = get_table(tree_xml, root_xml, reserved_words)\n",
        "    from IPython.display import display\n",
        "    #display(df)\n",
        "    #Gets trained decision tree\n",
        "    dtc = get_trained_tree(tree_file_folder)\n",
        "    #Predicts types and concat to the phrases table\n",
        "    types = predict_types(dtc, df)\n",
        "    df = concat_types_to_df(df, types)\n",
        "    #Updates and writes new xml\n",
        "    for use_case in root_xml:\n",
        "      for flow in use_case:\n",
        "        for phrase in flow:\n",
        "          if phrase.get('type') != None:\n",
        "            continue\n",
        "          try:\n",
        "            phrase.set('type', df.loc[(df['id_use_case']==use_case.get(\"id\")) & (df['id_phrase']==phrase.get(\"id\")) & (df['flow']==flow.tag), 'types'].iloc[0])\n",
        "          except Exception as e:\n",
        "            print(e)\n",
        "    # if i==0:\n",
        "    #   tree_xml.write('predicted_phrases_types.xml')\n",
        "    # else:\n",
        "    #   tree_xml.write('predicted_phrases_types('+ str(i) +').xml')\n",
        "    # i+=1\n",
        "    tree_xml.write(name_file(save_folder, xml_file))\n",
        "\n",
        "    predicted_xmls.append(tree_xml)\n",
        "  return predicted_xmls"
      ],
      "metadata": {
        "id": "8Op1s0J4-8bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predicting"
      ],
      "metadata": {
        "id": "PwzbODFl7BMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reserved words\n",
        "reserved_words = os.environ.get(\"RESERVED_WORDS\")\n",
        "if reserved_words == None or reserved_words == \"\":\n",
        "  reserved_words = ['Informar', 'Inserir']\n",
        "else:\n",
        "  reserved_words = reserved_words.split(\";\")"
      ],
      "metadata": {
        "id": "oQaGxNUY7AVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Folder where the **decision three model** was uploaded\n",
        "tree_file_folder = os.environ.get(\"DECISION_TREE_DIR\")\n",
        "if tree_file_folder == None or tree_file_folder == \"\":\n",
        "  tree_file_folder = \"/content/\""
      ],
      "metadata": {
        "id": "a6EUwZ-52irG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Folder where the **xml files** of the phrases was uploaded\n",
        "tokenized_folder = os.environ.get(\"TOKENIZED_XML_DIR\")\n",
        "if tokenized_folder == None or tokenized_folder == \"\":\n",
        "  tokenized_folder = \"/content/\""
      ],
      "metadata": {
        "id": "fHkomGxx4dfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Folder where the **predicted xml files** are going to be saved\n",
        "save_folder = os.environ.get(\"PREDICTED_XML_DIR\")\n",
        "if save_folder == None or save_folder == \"\":\n",
        "  save_folder = \"/content/predicted_files/\"\n",
        "os.makedirs(save_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "t5KVpa8C7ILe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run ***predict()***\n",
        "predicted_xmls = predict(tree_file_folder, reserved_words, tokenized_folder, save_folder)\n",
        "\n",
        "# for xml in predicted_xmls:\n",
        "#   print(parseString(ET.tostring(xml.getroot())).toprettyxml(indent='\\t', newl='\\n'))"
      ],
      "metadata": {
        "id": "sxinQXW27PAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}