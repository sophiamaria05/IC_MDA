{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophiamaria05/IC_MDA/blob/update-code-to-first-official-version/Tokenize_xml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import html"
      ],
      "metadata": {
        "id": "f_MULCMIMSGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the libraries and installing dependancies"
      ],
      "metadata": {
        "id": "ioyYidHfKlOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if os.environ.get(\"TOKENIZE_PIP_ALREADY_RUN\") != \"1\":\n",
        "  print(\"\\n\\nRunning Pip commands...\\n\\n\")\n",
        "\n",
        "  import subprocess\n",
        "  import sys\n",
        "\n",
        "  subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"h5py\"])\n",
        "  subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"typing-extensions\"])\n",
        "  subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"wheel\"])\n",
        "  os.environ[\"TOKENIZE_PIP_ALREADY_RUN\"] = \"1\"\n",
        "else:\n",
        "    print(\"\\n\\nPip commands were already executed!\\n\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dApDmh5nxF7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNHJPqLkPrId"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom.minidom import parseString"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the functions"
      ],
      "metadata": {
        "id": "GFHfT0WyK5yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_nmod(dep):\n",
        "  nmod = 0\n",
        "  for i in range(0, len(dep)):\n",
        "    if dep[i]==\"nmod\":\n",
        "      nmod+=1\n",
        "  return nmod"
      ],
      "metadata": {
        "id": "fKfmSOIZOShG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_phrase(phrase):\n",
        "  try:\n",
        "    nodes = phrase.text.split()\n",
        "\n",
        "    nodes = [item.replace(\",\", \"\") for item in nodes]\n",
        "    remove_nodes = {\"e\", \"o\", \"a\", \",\", \"de\", \"do\", \"da\", \"para\"}\n",
        "    nodes[:] = [item for item in nodes if item not in remove_nodes]\n",
        "\n",
        "    if len(nodes) < 3:\n",
        "      print(\"INVALID PHRASE: \"+phrase.text)\n",
        "      return False\n",
        "  except Exception as e:\n",
        "    print(\"PHRASE NULL\")\n",
        "    return False\n",
        "\n",
        "  ET.SubElement(phrase, \"noun\").text = nodes[0].title()\n",
        "  ET.SubElement(phrase, \"verb\").text = nodes[1].title()\n",
        "\n",
        "  if (nodes[1].title() == \"Informa\") or (nodes[1].title() == \"Informar\"):\n",
        "    ET.SubElement(phrase, \"msg\").text = phrase.text.split(nodes[1], 1)[1]\n",
        "    return True\n",
        "\n",
        "  ET.SubElement(phrase, \"object\").text = nodes[2].title()\n",
        "\n",
        "  nodes = [item for item in nodes if item not in set(nodes[:2])][1:]\n",
        "  for i in range(0, len(nodes)):\n",
        "    complement = ET.SubElement(phrase, \"complement\")\n",
        "    complement.text = nodes[i].title()\n",
        "    complement.set('id', str(i))\n",
        "\n",
        "  return True"
      ],
      "metadata": {
        "id": "RzCQejCmlagA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_xml_files(xml_file_folder):\n",
        "  \"\"\"\n",
        "    Gets the folder path where the xml file with predicted phrases.\n",
        "    Returns a List[xml_files].\n",
        "  \"\"\"\n",
        "  return [file for file in os.listdir(xml_file_folder) if file.endswith('.xml')]"
      ],
      "metadata": {
        "id": "23Z5ZT_nOFtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(file_path, write_name):\n",
        "  xml = ET.parse(file_path, parser=ET.XMLParser(encoding='utf-8'))\n",
        "  xml_fix = ET.tostring(xml.getroot(), encoding='utf-8').decode('utf-8')\n",
        "  root = ET.fromstring(xml_fix)\n",
        "  xml_string = parseString(xml_fix)\n",
        "\n",
        "  for use_case in root:\n",
        "    for flow in use_case:\n",
        "      for phrase in flow:\n",
        "        tokenize_phrase(phrase)\n",
        "\n",
        "  tree = ET.ElementTree(root)\n",
        "  tree.write(write_name)\n",
        "  print(\">>\", file_path, \"<< TOKENIZED AND SAVED ON >>\", write_name, \"<<\")\n",
        "  print(html.unescape(parseString(ET.tostring(tree.getroot())).toprettyxml(indent='\\t', newl='\\n', )))"
      ],
      "metadata": {
        "id": "yOxTTHSKNabk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Runing the tokenization"
      ],
      "metadata": {
        "id": "LpKmAmt3K0Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder = os.environ.get(\"TO_BE_TOKENIZED_DIR\")\n",
        "if folder == None or folder == \"\":\n",
        "  folder = \"/content/\"\n",
        "\n",
        "save_folder = os.environ.get(\"TOKENIZED_XML_DIR\")\n",
        "if save_folder == None or save_folder == \"\":\n",
        "  save_folder = \"/content/tokenized/\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "files = get_xml_files(folder)\n",
        "for file_name in files:\n",
        "  tokenize(folder+\"/\"+file_name, save_folder+\"tokenized_phrases (\"+file_name.split('.', 1)[0]+\").xml\")"
      ],
      "metadata": {
        "id": "fMAzM-RZy3e0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM3dR/cJMLvUsAR9hXv1bK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}